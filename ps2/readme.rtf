{\rtf1\ansi\ansicpg1252\cocoartf1265\cocoasubrtf190
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 ArialMT;}
{\colortbl;\red255\green255\blue255;\red16\green60\blue192;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 Hao Wu 0532479\
Wei Xu 0525157\
\
1.Explain what the program can do?\
\
This program will first parse the downloaded web pages(many of them). Extract all the words that are in the pages, including all the words in all the tags, including <p> tag, title tag, header tag, etc.\
After that, we create some text files (let\'92s call them raw files) to store the words and also the document id associate with it. We write one raw file for one data file. For example, In the data set folder we have 100_data, 100_index, what we create is 100_txt. We can see that in the raw data file, they contains many duplicate tuples. For example, if cat appears in document No.1 many times, This tuples will appears multiple times in the raw text file.\
\
After we had the \'93raw files\'94, we use unix sort to sort the raw data files into text file name sort_XX(0-41).txt\
Which will sort the raw data (remove the duplicate tuples so much smaller). Now within the sort files, we get all the tuples(word and document id). \
\
After that, we can merge (\'93unix merge\'94) all the sorted files into a file named merge.rpt, The will contains all the tuples(word document id) in sorted order of all the files. \
\
Now we can build the inverted lists of words. Since the whole inverted lists for all the data are very long. We separated the whole inverted list into different file. 10000 lines per file. \
\
We also build lexicon dictionary which will contains all the words with their corresponding inverted list file and also the line(location of that file).\
\
We also build a docID-to-URL table which will help us to find the url given the particular document id. \
\
\
2. How to use?\
First we should download all the raw files that you want to feed in this program on the folder on the same level of the the run_dir. like that:\
run_dir    (vol_0_99.tar  \'85 vol_4100_4179.tar)\
\
Open the terminal, we wrote a little shell to make the life a little bit easier:)\
go to the run_dir directory, and just run the sort.sh and the program will do all the work and get the results.\
when the console ask the user:\
%Please input start number: \
input the start num: for example, 0\
%Please input stop number:\
output the end num: for example 100\
\
That will specific the range of the html raw files you want to evaluate.\
\
Note: make sure that the machine should had zlib and g++ installed. \
\
3. How does the program do?\
We download the whole data and test on our program, it takes about 20 min to output the results. \
The docID_url.rpt is the file that contains all the document id and their corresponding urls, the size of the file is 165.7MB. The inverted list is the file contains word and its documents, there are 25 of them(for the total data) each of them the size is 1.1 MB. We also had the lexicon.rpt which has the size of 6.9 MB.\
\
\
4. More details:\
(a)HTML parsing, we write c program (parser.h, parser.c). \
The usage of the the parser. \
function to parse urls and pages into words\
\
/***************************************************************\
function to parse urls and pages into words\
\
return value:\
  a) >= 0, if success;\
  b) -1, if buffer (for words) too small\
\
arguments:\
  1) page to be parsed;\
  2) buffer to store parsed out words and their contexts;\
     format: "<WORD> <CONTEXT>\\n"\
     <CONTEXT>: 'B', bold; 'H', head (h1-h6); 'I', italic;\
                'P', plain; 'T', title; 'U', URL.\
	4) length of buffer\
\
***************************************************************/\
\
(b) For indexing:\
\pard\pardeftab720\sa240
\cf0 format of the index file:\
\pard\pardeftab720\sa240
{\field{\*\fldinst{HYPERLINK "http://0010024.e-xpert.co.nz/SITE_Default/SITE_ema/search.asp"}}{\fldrslt 
\f1\fs26 \cf2 \ul \ulc2 www.}}
\f1\fs26 \cf2 \ul \ulc2 cnn,com\cf0 \ulnone 1 0 2161 203.110.28.35 80 ok\
first entry is the URL, the 4th entry is the length of the page in bytes, and the last entry is the return code (which could be ok or 404 or 403, etc). To get the starting offset in bytes for the 5th page, we need to add up the lengths of pages 1 to 4.
\f0\fs24  \
We didn\'92t uncompressed the data and the index to read them, we used zlib library to read the data accounting to the length that the index provided. Once we get a particular page, we can immediately parse the html and get all the useful words. \
Once we get all the words, we can write it to the \'93raw files\'94 like 0.txt, 99.txt, which will be sorted in the next step. \
\
(c) We used unix sort to sort all the \'93raw files\'94 :\
we sort all the txt file in the dir and out put it as a sort_XX.txt, we only keep the unique tuples. \
sort -u $dir/*txt > sort_$\{i\}.txt\
After this step, we get all the sorted files which will be merged in the next step.\
\
(d)The merge step:\
We also used the unix sort(but this time only do the merge, not to sort any data) into the file we named it merge.rpt. \
using this command line:\
sort -m sort_*.txt > merge.rpt\
in the next step, we will used this file to create the inverted list.\
\
(e) Create the inverted lists and look up. \
We used the merge.cc to create the inverted list, while create the lexicon dictionary map at the same time. We output the inverted list into different files(ex. merge_0.map contains all the words from \'91a\'92 to \'91b\'92) the limit of each inverted list file is 10000 lines. We also create the lexicon dictionary at the same time, for example: \'93book merge_6.map 8904\'94 means that the word \'93book\'94 the inverted list for that word is in the file merge_6.map and line No.8904. \
for example, in the merge_6.map file, line 8904 it shows like this:\
book 1000508 1000509 1001236 1005538 1005539 1005733 1007401 1007402 \'85.\
so the \'93book\'94 contains in document id: 1000508 1000509\'85.\
We can look it up in the docID_url file:\
1000508 www.aus.org.nz/pay_conditions/2004Agreements/VictoriaGeneral04-05.htm\
it will tells us the url of the docID which is:\
www.aus.org.nz/pay_conditions/2004Agreements/VictoriaGeneral04-05.htm\
In conclusion:\
1. This program can be run on all the linux system and Mac OS. \
2. We didn\'92t uncompress all the data, and read them directly. Thus the speed of the program is really fast. \
3. We save all the data in binary format which will speed up the program. \
\
Thank you!\
\
Hao Wu\
Wei Xu}